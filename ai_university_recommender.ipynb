{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfeDBVRRqYt3xPFGFLOHZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajShah3006/Saarthi/blob/main/ai_university_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cad82c67"
      },
      "source": [
        "!pip install requests beautifulsoup4 google-generativeai gradio scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import concurrent.futures\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gradio as gr\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except Exception:\n",
        "    print(\"‚ö†Ô∏è API Key check failed. Ensure it is in secrets.\")\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "# --- 2. CACHING & UTILS ---\n",
        "CACHE_FILE = \"university_data_cached.json\"\n",
        "\n",
        "def save_data(data):\n",
        "    with open(CACHE_FILE, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "    print(\"üíæ Database saved.\")\n",
        "\n",
        "def load_data():\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        with open(CACHE_FILE, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            # Auto-Fix names if they are missing\n",
        "            for p in data:\n",
        "                if 'program_name' not in p and 'name' in p:\n",
        "                    p['program_name'] = p['name']\n",
        "            print(f\"‚ö° Loaded {len(data)} programs from cache.\")\n",
        "            return data\n",
        "    return None\n",
        "\n",
        "def get_embedding(text):\n",
        "    clean_text = re.sub(r'\\s+', ' ', str(text)).strip()[:2000]\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            result = genai.embed_content(\n",
        "                model=\"models/text-embedding-004\",\n",
        "                content=clean_text,\n",
        "                task_type=\"retrieval_document\"\n",
        "            )\n",
        "            return result['embedding']\n",
        "        except:\n",
        "            time.sleep(1)\n",
        "    return [0] * 768\n",
        "\n",
        "# --- 3. SCRAPING FUNCTIONS ---\n",
        "def list_all_programs(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        programs_list = []\n",
        "        container = soup.select_one('div.results.results-programs')\n",
        "        if not container: return None\n",
        "\n",
        "        program_elements = container.select('h2.result-heading')\n",
        "        for program_element in program_elements:\n",
        "            program_name = program_element.get_text(strip=True)\n",
        "            anchor_tag = program_element.find('a', href=True)\n",
        "            if anchor_tag:\n",
        "                programs_list.append({'name': program_name, 'url': anchor_tag['href']})\n",
        "        return programs_list\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def scrape_university_info(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        data = {}\n",
        "\n",
        "        prerequisites = []\n",
        "        headings = soup.find_all(string=re.compile(r'Prerequisites|Admission Requirements', re.IGNORECASE))\n",
        "        for h in headings:\n",
        "            lst = h.find_next(['ul', 'ol'])\n",
        "            if lst: prerequisites.extend([li.get_text(strip=True) for li in lst.select('li')])\n",
        "\n",
        "        if prerequisites: data['prerequisites'] = \", \".join(list(set(prerequisites)))\n",
        "\n",
        "        avg = soup.find(string=re.compile(r'\\d+%', re.IGNORECASE))\n",
        "        if avg: data['admission_average'] = avg.strip()\n",
        "\n",
        "        return data\n",
        "    except:\n",
        "        return {}\n",
        "\n",
        "# --- 4. MAIN EXECUTION (Scrape + Embed) ---\n",
        "all_programs_detailed_data = load_data()\n",
        "\n",
        "if not all_programs_detailed_data:\n",
        "    print(\"üöÄ No cache found. Starting FRESH SCRAPE...\")\n",
        "\n",
        "    # A. Get URLs\n",
        "    programs_with_urls = []\n",
        "    alphabet_groups = ['a', 'b', 'c', 'd-e', 'f-g', 'h', 'i', 'j-l', 'm', 'n-p', 'q-s', 't-z']\n",
        "    for group in alphabet_groups:\n",
        "        res = list_all_programs(f\"https://www.ouinfo.ca/programs/search/?search=&group={group}\")\n",
        "        if res: programs_with_urls.extend(res)\n",
        "    print(f\"‚úÖ Found {len(programs_with_urls)} programs.\")\n",
        "\n",
        "    # B. Scrape Details (Parallel)\n",
        "    print(\"‚è≥ Scraping details (Parallel)...\")\n",
        "    scraped_results = []\n",
        "\n",
        "    def process_program(entry):\n",
        "        url = f\"https://www.ouinfo.ca{entry['url']}\"\n",
        "        data = scrape_university_info(url)\n",
        "        return {\n",
        "            'program_name': entry['name'],\n",
        "            'program_url': url,\n",
        "            **data\n",
        "        }\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(process_program, p): p for p in programs_with_urls}\n",
        "        completed = 0\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result: scraped_results.append(result)\n",
        "            completed += 1\n",
        "            if completed % 50 == 0: print(f\"{completed}...\", end=\"\")\n",
        "\n",
        "    # C. Embeddings (Sequential to avoid Timeout)\n",
        "    print(\"\\nüß† Generating AI Embeddings (One-by-one to prevent timeout)...\")\n",
        "    all_programs_detailed_data = []\n",
        "\n",
        "    for i, item in enumerate(scraped_results):\n",
        "        text = f\"{item['program_name']} {item.get('prerequisites', '')}\"\n",
        "        item['embedding'] = get_embedding(text)\n",
        "        all_programs_detailed_data.append(item)\n",
        "\n",
        "        if i % 25 == 0: print(\".\", end=\"\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "    save_data(all_programs_detailed_data)\n",
        "\n",
        "# --- 5. CHATBOT & GRADIO ---\n",
        "def find_best_matches(user_query, all_data, top_k=8):\n",
        "    query_vector = get_embedding(user_query)\n",
        "    valid_data = [p for p in all_data if 'embedding' in p]\n",
        "    if not valid_data: return all_data[:5]\n",
        "\n",
        "    db_vectors = [p['embedding'] for p in valid_data]\n",
        "    scores = cosine_similarity([query_vector], db_vectors)[0]\n",
        "    top_indices = scores.argsort()[-top_k:][::-1]\n",
        "    return [valid_data[i] for i in top_indices]\n",
        "\n",
        "def generate_chatbot_response(user_data, relevant_programs):\n",
        "    context = \"\"\n",
        "    for p in relevant_programs:\n",
        "        name = p.get('program_name', 'Unknown Program')\n",
        "        url = p.get('program_url', '#')\n",
        "        avg = p.get('admission_average', 'Not Listed')\n",
        "        prereqs = p.get('prerequisites', 'Not listed') # Grab Prereqs for the prompt\n",
        "\n",
        "        context += f\"- {name}\\n\"\n",
        "        context += f\"  Target Avg: {avg}\\n\"\n",
        "        context += f\"  Required Prereqs: {prereqs}\\n\"\n",
        "        context += f\"  Link: {url}\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Act as 'Saarthi', a wise, encouraging university guidance counselor.\n",
        "\n",
        "    STUDENT PROFILE:\n",
        "    - Interests: {user_data['intrests']}\n",
        "    - Current Grade Level: {user_data['grade']}\n",
        "    - Estimated Average: {user_data['overall_average']}\n",
        "    - Current Subjects: {user_data['subjects']}\n",
        "    - Location: {user_data['location']}\n",
        "\n",
        "    TOP DATABASE MATCHES:\n",
        "    {context}\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. **Rank & Recommend:** Recommend the top 10 programs from the list above.\n",
        "    2. **Subject Check (CRITICAL):** Compare the student's \"Current Subjects\" against the \"Required Prereqs\".\n",
        "       - If they are missing a key subject, warn them politely!\n",
        "       - If their subjects look perfect, tell them they are on the right track.\n",
        "    3. **Fit Analysis:** Explain why these programs fit their interest in \"{user_data['intrests']}\".\n",
        "    4. **Extracurriculars:** Suggest specific side projects or clubs based on their interests.\n",
        "    5. **Tone:** Be warm, supportive, and use emojis.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def web_wrapper(subjects, interests, average, grade, location):\n",
        "    user_data = {\n",
        "        'subjects': subjects, 'intrests': interests,\n",
        "        'overall_average': average, 'grade': grade, 'location': location\n",
        "    }\n",
        "    matches = find_best_matches(interests, all_programs_detailed_data)\n",
        "    return generate_chatbot_response(user_data, matches)\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=web_wrapper,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Current Subjects (e.g. Calculus, Physics, English)\"),\n",
        "        gr.Textbox(label=\"Interests\"),\n",
        "        gr.Textbox(label=\"Overall Average%\"),\n",
        "        gr.Textbox(label=\"What grade are you in?\"),\n",
        "        gr.Textbox(label=\"Location\")\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"Saarthi Advice\"),\n",
        "    title=\"Saarthi AI\",\n",
        "    description=\"Running locally in Colab.\"\n",
        ")\n",
        "\n",
        "interface.launch(inline=True, share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "E5_QpebQ0B_z",
        "outputId": "d1be4062-25d3-4274-c0a6-003cf9ab2593"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Loaded 1399 programs from cache.\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a27b505ec8fa417d8e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a27b505ec8fa417d8e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a27b505ec8fa417d8e.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import concurrent.futures\n",
        "import json\n",
        "import os\n",
        "import csv\n",
        "import datetime\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gradio as gr\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except Exception:\n",
        "    print(\"‚ö†Ô∏è API Key check failed. Ensure it is in secrets.\")\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "# --- 2. LOGGING & CACHING ---\n",
        "CACHE_FILE = \"university_data_cached.json\"\n",
        "LOG_FILE = \"user_traffic_logs.csv\"\n",
        "\n",
        "if not os.path.exists(LOG_FILE):\n",
        "    with open(LOG_FILE, mode='w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Timestamp\", \"Grade\", \"Location\", \"Interests\", \"Subjects\"])\n",
        "\n",
        "def log_interaction(grade, location, interests, subjects):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"\\nüîî [ALERT {timestamp}] New User from {location}!\")\n",
        "    with open(LOG_FILE, mode='a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([timestamp, grade, location, interests, subjects])\n",
        "\n",
        "def save_data(data):\n",
        "    with open(CACHE_FILE, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "    print(\"üíæ Database saved.\")\n",
        "\n",
        "def load_data():\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        with open(CACHE_FILE, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            for p in data:\n",
        "                if 'program_name' not in p and 'name' in p:\n",
        "                    p['program_name'] = p['name']\n",
        "            print(f\"‚ö° Loaded {len(data)} programs from cache.\")\n",
        "            return data\n",
        "    return None\n",
        "\n",
        "def get_embedding(text):\n",
        "    clean_text = re.sub(r'\\s+', ' ', str(text)).strip()[:2000]\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            result = genai.embed_content(\n",
        "                model=\"models/text-embedding-004\",\n",
        "                content=clean_text,\n",
        "                task_type=\"retrieval_document\"\n",
        "            )\n",
        "            return result['embedding']\n",
        "        except:\n",
        "            time.sleep(1)\n",
        "    return [0] * 768\n",
        "\n",
        "# --- 3. SCRAPING FUNCTIONS ---\n",
        "def list_all_programs(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        programs_list = []\n",
        "        container = soup.select_one('div.results.results-programs')\n",
        "        if not container: return None\n",
        "        program_elements = container.select('h2.result-heading')\n",
        "        for program_element in program_elements:\n",
        "            program_name = program_element.get_text(strip=True)\n",
        "            anchor_tag = program_element.find('a', href=True)\n",
        "            if anchor_tag:\n",
        "                programs_list.append({'name': program_name, 'url': anchor_tag['href']})\n",
        "        return programs_list\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def scrape_university_info(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        data = {}\n",
        "        prerequisites = []\n",
        "        headings = soup.find_all(string=re.compile(r'Prerequisites|Admission Requirements', re.IGNORECASE))\n",
        "        for h in headings:\n",
        "            lst = h.find_next(['ul', 'ol'])\n",
        "            if lst: prerequisites.extend([li.get_text(strip=True) for li in lst.select('li')])\n",
        "        if prerequisites: data['prerequisites'] = \", \".join(list(set(prerequisites)))\n",
        "        avg = soup.find(string=re.compile(r'\\d+%', re.IGNORECASE))\n",
        "        if avg: data['admission_average'] = avg.strip()\n",
        "        return data\n",
        "    except:\n",
        "        return {}\n",
        "\n",
        "# --- 4. MAIN EXECUTION ---\n",
        "all_programs_detailed_data = load_data()\n",
        "\n",
        "if not all_programs_detailed_data:\n",
        "    print(\"üöÄ No cache found. Starting FRESH SCRAPE...\")\n",
        "    programs_with_urls = []\n",
        "    alphabet_groups = ['a', 'b', 'c', 'd-e', 'f-g', 'h', 'i', 'j-l', 'm', 'n-p', 'q-s', 't-z']\n",
        "    for group in alphabet_groups:\n",
        "        res = list_all_programs(f\"https://www.ouinfo.ca/programs/search/?search=&group={group}\")\n",
        "        if res: programs_with_urls.extend(res)\n",
        "    print(f\"‚úÖ Found {len(programs_with_urls)} programs.\")\n",
        "\n",
        "    scraped_results = []\n",
        "    def process_program(entry):\n",
        "        url = f\"https://www.ouinfo.ca{entry['url']}\"\n",
        "        data = scrape_university_info(url)\n",
        "        return {'program_name': entry['name'], 'program_url': url, **data}\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(process_program, p): p for p in programs_with_urls}\n",
        "        completed = 0\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result: scraped_results.append(result)\n",
        "            completed += 1\n",
        "            if completed % 50 == 0: print(f\"{completed}...\", end=\"\")\n",
        "\n",
        "    print(\"\\nüß† Generating AI Embeddings...\")\n",
        "    all_programs_detailed_data = []\n",
        "    for i, item in enumerate(scraped_results):\n",
        "        text = f\"{item['program_name']} {item.get('prerequisites', '')}\"\n",
        "        item['embedding'] = get_embedding(text)\n",
        "        all_programs_detailed_data.append(item)\n",
        "        if i % 25 == 0: print(\".\", end=\"\")\n",
        "        time.sleep(0.1)\n",
        "    save_data(all_programs_detailed_data)\n",
        "\n",
        "# --- 5. CHATBOT & GRADIO ---\n",
        "def find_best_matches(user_query, all_data, top_k=8):\n",
        "    query_vector = get_embedding(user_query)\n",
        "    valid_data = [p for p in all_data if 'embedding' in p]\n",
        "    if not valid_data: return all_data[:5]\n",
        "    db_vectors = [p['embedding'] for p in valid_data]\n",
        "    scores = cosine_similarity([query_vector], db_vectors)[0]\n",
        "    top_indices = scores.argsort()[-top_k:][::-1]\n",
        "    return [valid_data[i] for i in top_indices]\n",
        "\n",
        "def generate_chatbot_response(user_data, relevant_programs):\n",
        "    context = \"\"\n",
        "    for p in relevant_programs:\n",
        "        name = p.get('program_name', 'Unknown Program')\n",
        "        url = p.get('program_url', '#')\n",
        "        avg = p.get('admission_average', 'Not Listed')\n",
        "        prereqs = p.get('prerequisites', 'Not listed')\n",
        "        context += f\"- PROGRAM: {name}\\n  AVG: {avg}\\n  PREREQS: {prereqs}\\n  LINK: {url}\\n\\n\"\n",
        "\n",
        "    # --- THIS IS THE UPDATED PROMPT FOR COMMUTE LOGIC ---\n",
        "    prompt = f\"\"\"\n",
        "    Act as 'Saarthi', a wise university guidance counselor in Ontario.\n",
        "\n",
        "    STUDENT PROFILE:\n",
        "    - Interest: {user_data['intrests']}\n",
        "    - Grade: {user_data['grade']}\n",
        "    - Avg: {user_data['overall_average']}\n",
        "    - Subjects: {user_data['subjects']}\n",
        "    - HOME LOCATION: {user_data['location']}\n",
        "\n",
        "    TOP MATCHES:\n",
        "    {context}\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. Recommend the top 3 programs.\n",
        "    2. **Prerequisite Check:** Compare their subjects to the requirements. Warn if missing.\n",
        "    3. **COMMUTE ANALYSIS (Crucial):** For each recommended university, calculate the estimated travel time from '{user_data['location']}'.\n",
        "       - **Time:** Estimate the one-way time (e.g. \"45 mins\").\n",
        "       - **Mode:** Suggest the best way (GO Train, TTC, Bus, or Car).\n",
        "       - **Cost:** Estimate monthly cost (e.g. \"GO Train is approx $250/month\" or \"Gas is approx $200/month\").\n",
        "       - **Verdict:** If the commute is > 1 hour one-way, strongly recommend **RESIDENCE**. If < 45 mins, recommend **COMMUTING**.\n",
        "\n",
        "    4. **Tone:** Warm and supportive. Use emojis üöå üè† üéì.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def web_wrapper(subjects, interests, average, grade, location):\n",
        "    log_interaction(grade, location, interests, subjects)\n",
        "\n",
        "    user_data = {\n",
        "        'subjects': subjects, 'intrests': interests,\n",
        "        'overall_average': average, 'grade': grade, 'location': location\n",
        "    }\n",
        "    matches = find_best_matches(interests, all_programs_detailed_data)\n",
        "    return generate_chatbot_response(user_data, matches)\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=web_wrapper,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Current Subjects\"),\n",
        "        gr.Textbox(label=\"Interests\"),\n",
        "        gr.Textbox(label=\"Overall Average%\"),\n",
        "        gr.Textbox(label=\"Grade\"),\n",
        "        gr.Textbox(label=\"Location (City, ON)\"), # Updated Label\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"Saarthi Advice\"),\n",
        "    title=\"Saarthi AI: Commute & Program Guide\",\n",
        "    description=\"I calculate commute times, costs, and residency options for you!\"\n",
        ")\n",
        "\n",
        "interface.launch(inline=True, share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "h_tQS979_cOl",
        "outputId": "139bc89b-6fb4-49fa-e300-4fdbd14f70f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° Loaded 1399 programs from cache.\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e0383c1c3be35e8c67.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e0383c1c3be35e8c67.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîî [ALERT 2025-11-30 04:23:41] New User from Oakville!\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://e0383c1c3be35e8c67.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}