{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJwKdnza/q4XT1GZHCsmtN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajShah3006/Saarthi/blob/main/ai_university_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cad82c67"
      },
      "source": [
        "!pip install requests beautifulsoup4 google-generativeai gradio scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from google.colab import drive # IMPORT DRIVE\n",
        "import time\n",
        "import concurrent.futures\n",
        "import json\n",
        "import os\n",
        "import csv\n",
        "import datetime\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gradio as gr\n",
        "\n",
        "# --- 1. SETUP & DRIVE MOUNT ---\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except Exception:\n",
        "    print(\"âš ï¸ API Key check failed. Ensure it is in secrets.\")\n",
        "\n",
        "# MOUNT GOOGLE DRIVE (The Permanent Storage)\n",
        "print(\"ðŸ“‚ Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a folder for your project in Drive if it doesn't exist\n",
        "DRIVE_FOLDER = \"/content/drive/My Drive/Saarthi_Project_Data\"\n",
        "if not os.path.exists(DRIVE_FOLDER):\n",
        "    os.makedirs(DRIVE_FOLDER)\n",
        "\n",
        "# Update paths to save inside Google Drive\n",
        "CACHE_FILE = f\"{DRIVE_FOLDER}/university_data_cached.json\"\n",
        "LOG_FILE = f\"{DRIVE_FOLDER}/user_traffic_logs.csv\"\n",
        "\n",
        "print(f\"âœ… Data will be saved permanently to: {CACHE_FILE}\")\n",
        "\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "# --- 2. LOGGING & CACHING ---\n",
        "\n",
        "if not os.path.exists(LOG_FILE):\n",
        "    with open(LOG_FILE, mode='w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Timestamp\", \"Grade\", \"Location\", \"Interests\", \"Subjects\"])\n",
        "\n",
        "def log_interaction(grade, location, interests, subjects):\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"\\nðŸ”” [ALERT {timestamp}] New User from {location}!\")\n",
        "    with open(LOG_FILE, mode='a', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([timestamp, grade, location, interests, subjects])\n",
        "\n",
        "def save_data(data):\n",
        "    with open(CACHE_FILE, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "    print(\"ðŸ’¾ Database saved to Google Drive.\")\n",
        "\n",
        "def load_data():\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        with open(CACHE_FILE, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            # Auto-Fix names if they are missing\n",
        "            for p in data:\n",
        "                if 'program_name' not in p and 'name' in p:\n",
        "                    p['program_name'] = p['name']\n",
        "            print(f\"âš¡ Loaded {len(data)} programs from Google Drive.\")\n",
        "            return data\n",
        "    return None\n",
        "\n",
        "# --- NEW: BATCH EMBEDDING (The Speed Fix) ---\n",
        "def get_batch_embeddings(text_list):\n",
        "    \"\"\"\n",
        "    Sends a list of texts to Google API in one go.\n",
        "    Much faster than sending them one by one.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Google text-embedding-004 supports batching\n",
        "        result = genai.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            content=text_list,\n",
        "            task_type=\"retrieval_document\"\n",
        "        )\n",
        "        # The result['embedding'] will be a list of lists (vectors)\n",
        "        return result['embedding']\n",
        "    except Exception as e:\n",
        "        print(f\"Batch Error: {e}\")\n",
        "        # Fallback: return empty vectors if batch fails\n",
        "        return [[0]*768 for _ in range(len(text_list))]\n",
        "\n",
        "def get_single_embedding(text):\n",
        "    # Keep this for the User Query (search)\n",
        "    clean_text = re.sub(r'\\s+', ' ', str(text)).strip()[:2000]\n",
        "    try:\n",
        "        result = genai.embed_content(\n",
        "            model=\"models/text-embedding-004\",\n",
        "            content=clean_text,\n",
        "            task_type=\"retrieval_query\" # Note: Query type for search\n",
        "        )\n",
        "        return result['embedding']\n",
        "    except:\n",
        "        return [0] * 768\n",
        "\n",
        "# --- 3. SCRAPING FUNCTIONS ---\n",
        "def list_all_programs(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        programs_list = []\n",
        "        container = soup.select_one('div.results.results-programs')\n",
        "        if not container: return None\n",
        "        program_elements = container.select('h2.result-heading')\n",
        "        for program_element in program_elements:\n",
        "            program_name = program_element.get_text(strip=True)\n",
        "            anchor_tag = program_element.find('a', href=True)\n",
        "            if anchor_tag:\n",
        "                programs_list.append({'name': program_name, 'url': anchor_tag['href']})\n",
        "        return programs_list\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def scrape_university_info(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        data = {}\n",
        "        prerequisites = []\n",
        "        headings = soup.find_all(string=re.compile(r'Prerequisites|Admission Requirements', re.IGNORECASE))\n",
        "        for h in headings:\n",
        "            lst = h.find_next(['ul', 'ol'])\n",
        "            if lst: prerequisites.extend([li.get_text(strip=True) for li in lst.select('li')])\n",
        "        if prerequisites: data['prerequisites'] = \", \".join(list(set(prerequisites)))\n",
        "        avg = soup.find(string=re.compile(r'\\d+%', re.IGNORECASE))\n",
        "        if avg: data['admission_average'] = avg.strip()\n",
        "        return data\n",
        "    except:\n",
        "        return {}\n",
        "\n",
        "# --- 4. MAIN EXECUTION ---\n",
        "all_programs_detailed_data = load_data()\n",
        "\n",
        "if not all_programs_detailed_data:\n",
        "    print(\"ðŸš€ No cache found in Drive. Starting FRESH SCRAPE...\")\n",
        "    programs_with_urls = []\n",
        "    alphabet_groups = ['a', 'b', 'c', 'd-e', 'f-g', 'h', 'i', 'j-l', 'm', 'n-p', 'q-s', 't-z']\n",
        "    for group in alphabet_groups:\n",
        "        res = list_all_programs(f\"https://www.ouinfo.ca/programs/search/?search=&group={group}\")\n",
        "        if res: programs_with_urls.extend(res)\n",
        "    print(f\"âœ… Found {len(programs_with_urls)} programs.\")\n",
        "\n",
        "    print(\"â³ Scraping details (Parallel)...\")\n",
        "    scraped_results = []\n",
        "    def process_program(entry):\n",
        "        url = f\"https://www.ouinfo.ca{entry['url']}\"\n",
        "        data = scrape_university_info(url)\n",
        "        return {'program_name': entry['name'], 'program_url': url, **data}\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(process_program, p): p for p in programs_with_urls}\n",
        "        completed = 0\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result: scraped_results.append(result)\n",
        "            completed += 1\n",
        "            if completed % 50 == 0: print(f\"{completed}...\", end=\"\")\n",
        "\n",
        "    # --- BATCH EMBEDDING LOGIC (The Fast Way) ---\n",
        "    print(\"\\n\\nðŸ§  Generating AI Embeddings (BATCH MODE - 50x Faster)...\")\n",
        "\n",
        "    # 1. Prepare texts\n",
        "    texts_to_embed = []\n",
        "    for item in scraped_results:\n",
        "        # Create the rich text string\n",
        "        text = f\"{item['program_name']} {item.get('prerequisites', '')}\"\n",
        "        # Clean it slightly\n",
        "        text = re.sub(r'\\s+', ' ', str(text)).strip()[:2000]\n",
        "        texts_to_embed.append(text)\n",
        "\n",
        "    # 2. Process in Batches of 50\n",
        "    BATCH_SIZE = 50\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts_to_embed), BATCH_SIZE):\n",
        "        batch = texts_to_embed[i : i + BATCH_SIZE]\n",
        "        print(f\"Processing batch {i} to {i+BATCH_SIZE}...\", end=\"\\r\")\n",
        "\n",
        "        batch_vectors = get_batch_embeddings(batch)\n",
        "        all_embeddings.extend(batch_vectors)\n",
        "\n",
        "        # Small sleep to be polite to the API rate limit\n",
        "        time.sleep(1)\n",
        "\n",
        "    # 3. Assign back to data\n",
        "    all_programs_detailed_data = []\n",
        "    for i, item in enumerate(scraped_results):\n",
        "        item['embedding'] = all_embeddings[i]\n",
        "        all_programs_detailed_data.append(item)\n",
        "\n",
        "    save_data(all_programs_detailed_data)\n",
        "    print(\"\\nâœ… Done! Data saved to Drive.\")\n",
        "\n",
        "# --- 5. CHATBOT & GRADIO ---\n",
        "def find_best_matches(user_query, all_data, top_k=8):\n",
        "    query_vector = get_single_embedding(user_query)\n",
        "    valid_data = [p for p in all_data if 'embedding' in p]\n",
        "    if not valid_data: return all_data[:5]\n",
        "    db_vectors = [p['embedding'] for p in valid_data]\n",
        "    scores = cosine_similarity([query_vector], db_vectors)[0]\n",
        "    top_indices = scores.argsort()[-top_k:][::-1]\n",
        "    return [valid_data[i] for i in top_indices]\n",
        "\n",
        "def generate_chatbot_response(user_data, relevant_programs):\n",
        "    context = \"\"\n",
        "    for p in relevant_programs:\n",
        "        name = p.get('program_name', 'Unknown Program')\n",
        "        url = p.get('program_url', '#')\n",
        "        avg = p.get('admission_average', 'Not Listed')\n",
        "        prereqs = p.get('prerequisites', 'Not listed')\n",
        "        context += f\"- PROGRAM: {name}\\n  AVG: {avg}\\n  PREREQS: {prereqs}\\n  LINK: {url}\\n\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Act as 'Saarthi', a wise university guidance counselor in Ontario.\n",
        "\n",
        "    STUDENT PROFILE:\n",
        "    - Interest: {user_data['intrests']}\n",
        "    - Grade: {user_data['grade']}\n",
        "    - Avg: {user_data['overall_average']}\n",
        "    - Subjects: {user_data['subjects']}\n",
        "    - HOME LOCATION: {user_data['location']}\n",
        "\n",
        "    TOP MATCHES:\n",
        "    {context}\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. **Rank & Recommend:** Recommend the top 10 programs.\n",
        "    2. **Prerequsite Check:** Compare \"Subjects\" vs \"Prereqs\". Warn if missing.\n",
        "    3. **Fit Analysis:** Explain fit.\n",
        "    4. **Extracurriculars:** Suggest side projects.\n",
        "    5. **COMMUTE ANALYSIS:** - Calculate estimated travel time from '{user_data['location']}' to the university.\n",
        "       - If > 1 hour, recommend RESIDENCE.\n",
        "       - Estimate Cost (GO Train/Gas).\n",
        "    6. **Tone:** Warm and supportive. Use emojis.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "def web_wrapper(subjects, interests, average, grade, location):\n",
        "    log_interaction(grade, location, interests, subjects)\n",
        "\n",
        "    user_data = {\n",
        "        'subjects': subjects, 'intrests': interests,\n",
        "        'overall_average': average, 'grade': grade, 'location': location\n",
        "    }\n",
        "    matches = find_best_matches(interests, all_programs_detailed_data)\n",
        "    return generate_chatbot_response(user_data, matches)\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=web_wrapper,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Current Subjects\"),\n",
        "        gr.Textbox(label=\"Interests\"),\n",
        "        gr.Textbox(label=\"Overall Average%\"),\n",
        "        gr.Textbox(label=\"Grade\"),\n",
        "        gr.Textbox(label=\"Location (City, ON)\"),\n",
        "    ],\n",
        "    outputs=gr.Markdown(label=\"Saarthi Advice\"),\n",
        "    title=\"Saarthi AI: Commute & Program Guide\",\n",
        ")\n",
        "\n",
        "interface.launch(inline=True, share=True, debug=True)"
      ],
      "metadata": {
        "id": "BPqaIFY5QKcc",
        "outputId": "eb016fc2-60b8-45c3-a077-3ca0083b27c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ Connecting to Google Drive...\n",
            "Mounted at /content/drive\n",
            "âœ… Data will be saved permanently to: /content/drive/My Drive/Saarthi_Project_Data/university_data_cached.json\n",
            "âš¡ Loaded 1399 programs from Google Drive.\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://dfbb7001994c95c75e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dfbb7001994c95c75e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”” [ALERT 2025-12-01 19:53:55] New User from Vaughan, ON!\n",
            "\n",
            "ðŸ”” [ALERT 2025-12-01 19:54:26] New User from Vaughan, ON!\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://dfbb7001994c95c75e.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}