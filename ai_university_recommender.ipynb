{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6SpC+hRD38DbgkvqkNYoR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajShah3006/Saarthi/blob/main/ai_university_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cad82c67"
      },
      "source": [
        "!pip install -q google-generativeai"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time # Added specifically for the loop pacing\n",
        "\n",
        "# Assume GOOGLE_API_KEY is already set up in Colab secrets\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except Exception:\n",
        "    print(\"API Key check failed. Ensure it is in secrets.\")\n",
        "\n",
        "# Initialize the Gemini API\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "# --- YOUR ORIGINAL FUNCTIONS (UNCHANGED) ---\n",
        "\n",
        "def get_user_information(user_data=None):\n",
        "    \"\"\"Collects information from the user.\"\"\"\n",
        "    if user_data: return user_data\n",
        "\n",
        "    user_data = {}\n",
        "    user_data['subjects'] = input(\"What subjects are you taking currently? \")\n",
        "    user_data['intrests'] = input(\"What are your intrests? \")\n",
        "    user_data['overall_average'] = input(\"What is your overall average? \")\n",
        "    user_data['grade'] = input(\"What grade are you in? \")\n",
        "    user_data['location'] = input(\"Where are you located? \")\n",
        "    return user_data\n",
        "\n",
        "def list_all_programs(url):\n",
        "    \"\"\"Scrapes and lists all program names and their URLs from the given URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        programs_list = []\n",
        "\n",
        "        # The container class from your original code\n",
        "        container = soup.select_one('div.results.results-programs')\n",
        "\n",
        "        if not container:\n",
        "            # print(f\"Could not find container on {url}\")\n",
        "            return None\n",
        "\n",
        "        program_elements = container.select('h2.result-heading')\n",
        "        if not program_elements:\n",
        "            return None\n",
        "\n",
        "        for program_element in program_elements:\n",
        "            program_name = program_element.get_text(strip=True)\n",
        "            anchor_tag = program_element.find('a', href=True)\n",
        "            if anchor_tag:\n",
        "                program_url = anchor_tag['href']\n",
        "                programs_list.append({'name': program_name, 'url': program_url})\n",
        "\n",
        "        if programs_list:\n",
        "            return programs_list\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping list on {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_university_info(url):\n",
        "    \"\"\"Scrapes university information from a given URL.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        university_data = {}\n",
        "\n",
        "        program_element = soup.select_one('h1.program-title')\n",
        "        if program_element:\n",
        "            university_data['program'] = program_element.get_text(strip=True)\n",
        "\n",
        "        prerequisites = []\n",
        "        prereq_headings = soup.find_all(string=re.compile(r'Prerequisites|Admission Requirements', re.IGNORECASE))\n",
        "        for heading in prereq_headings:\n",
        "            list_element = heading.find_next(['ul', 'ol'])\n",
        "            if list_element:\n",
        "                list_items = [li.get_text(strip=True) for li in list_element.select('li')]\n",
        "                if list_items:\n",
        "                    prerequisites.extend(list_items)\n",
        "            else:\n",
        "                parent = heading.parent\n",
        "                if parent:\n",
        "                    text_content = parent.get_text(separator=' ', strip=True)\n",
        "                    if len(text_content) > len(heading.get_text(strip=True)):\n",
        "                        prerequisites.append(text_content)\n",
        "\n",
        "        if prerequisites:\n",
        "            university_data['prerequisites'] = \"\\n\".join(list(set(prerequisites)))\n",
        "\n",
        "        admission_average = None\n",
        "        average_text = soup.find(string=re.compile(r'(?:admission|minimum)?\\s*average.*?\\d+%', re.IGNORECASE))\n",
        "        if average_text:\n",
        "            admission_average = average_text.strip()\n",
        "        else:\n",
        "            average_element = soup.select_one('.admission-average-range, .average-grade')\n",
        "            if average_element:\n",
        "                admission_average = average_element.get_text(strip=True)\n",
        "\n",
        "        if admission_average:\n",
        "            university_data['admission_average'] = admission_average\n",
        "\n",
        "        # (Location logic omitted as per original code structure returning None for location usually)\n",
        "\n",
        "        if university_data:\n",
        "            return university_data\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error details: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_chatbot_response(user_data, all_programs_detailed_data):\n",
        "    \"\"\"Generates a chatbot response based on user data and scraped university info.\"\"\"\n",
        "    prompt = f\"\"\"Based on the following student information:\n",
        "- **Subjects:** {user_data['subjects']}\n",
        "- **Intrests:** {user_data['intrests']}\n",
        "- **Overall Average:** {user_data['overall_average']}\n",
        "- **Grade:** {user_data['grade']}\n",
        "\"\"\"\n",
        "    relevant_programs_info = \"\"\n",
        "\n",
        "    if all_programs_detailed_data:\n",
        "        relevant_programs_info += \"\\n**Information about potentially relevant programs:**\\n\\n\"\n",
        "\n",
        "        # LIMITER: Sending only first 30 to avoid token limits in this demo\n",
        "        for program_data in all_programs_detailed_data[:30]:\n",
        "            relevant_programs_info += f\"**Program Name:** {program_data.get('program_name', 'N/A')}\\n\"\n",
        "            relevant_programs_info += f\"**Program URL:** {program_data.get('program_url', 'N/A')}\\n\"\n",
        "            relevant_programs_info += f\"**Prerequisites:** {program_data.get('prerequisites', 'N/A')}\\n\"\n",
        "            relevant_programs_info += f\"**Admission Average:** {program_data.get('admission_average', 'N/A')}\\n\"\n",
        "            relevant_programs_info += \"---\\n\\n\"\n",
        "    else:\n",
        "        relevant_programs_info += \"Could not find detailed information for programs.\\n\\n\"\n",
        "\n",
        "    if relevant_programs_info:\n",
        "        prompt += relevant_programs_info\n",
        "    else:\n",
        "        prompt += \"\\nCould not retrieve detailed program information.\\n\"\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Please provide some relevant information, such as:\n",
        "- What program is recommended based on the student's interests and scraped program data\n",
        "- A ranking of relevant universities for that specific program\n",
        "- Recommendations for high school courses to pursue and projects to complete.\n",
        "Only give information for universities in Ontario.\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = chat.send_message(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error communicating with Gemini: {e}\"\n",
        "\n",
        "# --- MAIN EXECUTION FLOW ---\n",
        "\n",
        "print(\"Hello! I'm a student assistant chatbot.\")\n",
        "\n",
        "# STEP 2: Scrape program URLs by looping through Alphabet Groups\n",
        "# We need to hit every letter group to get the full list of 1394 programs.\n",
        "alphabet_groups = ['a', 'b', 'c', 'd-e', 'f-g', 'h', 'i', 'j-l', 'm', 'n-p', 'q-s', 't-z']\n",
        "programs_with_urls = []\n",
        "\n",
        "print(\"Step 2: Building Master List of Programs (This loops through A-Z)...\")\n",
        "\n",
        "for group in alphabet_groups:\n",
        "    group_url = f\"https://www.ouinfo.ca/programs/search/?search=&group={group}\"\n",
        "    print(f\"Fetching Group: {group.upper()}...\", end=\" \")\n",
        "\n",
        "    group_programs = list_all_programs(group_url)\n",
        "\n",
        "    if group_programs:\n",
        "        programs_with_urls.extend(group_programs)\n",
        "        print(f\"Found {len(group_programs)} programs.\")\n",
        "    else:\n",
        "        print(\"No programs found.\")\n",
        "\n",
        "print(f\"\\nTotal Programs Found: {len(programs_with_urls)}\")\n",
        "\n",
        "all_programs_detailed_data = []\n",
        "\n",
        "import concurrent.futures\n",
        "\n",
        "# STEP 3: Parallel Scraping (The Fast Way)\n",
        "if programs_with_urls:\n",
        "    print(f\"\\nStep 3: Starting parallel scrape of {len(programs_with_urls)} programs...\")\n",
        "\n",
        "    all_programs_detailed_data = []\n",
        "\n",
        "    # This function creates the data dictionary for a single program\n",
        "    def process_program(program_entry):\n",
        "        url = f\"https://www.ouinfo.ca{program_entry['url']}\"\n",
        "        scraped_data = scrape_university_info(url)\n",
        "        if scraped_data:\n",
        "            return {\n",
        "                'program_name': program_entry['name'],\n",
        "                'program_url': url,\n",
        "                **scraped_data\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    # We use 10 \"workers\" to scrape 10 pages at once.\n",
        "    # WARNING: Don't go above 10-15 or the website might ban you for attacking them.\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        # Submit all tasks\n",
        "        future_to_program = {executor.submit(process_program, p): p for p in programs_with_urls}\n",
        "\n",
        "        # Collect results as they finish\n",
        "        completed_count = 0\n",
        "        for future in concurrent.futures.as_completed(future_to_program):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                all_programs_detailed_data.append(result)\n",
        "\n",
        "            completed_count += 1\n",
        "            if completed_count % 50 == 0:\n",
        "                print(f\"Scraped {completed_count}/{len(programs_with_urls)} programs...\")\n",
        "\n",
        "    print(f\"\\nFinished. Successfully loaded {len(all_programs_detailed_data)} programs.\")\n",
        "else:\n",
        "    print(\"\\nFailed to retrieve the initial list of programs.\")\n",
        "\n",
        "\n",
        "# --- GRADIO WEB INTERFACE ---\n",
        "\n",
        "try:\n",
        "    import gradio as gr\n",
        "\n",
        "    def web_wrapper(subjects, interests, average, grade, location):\n",
        "        user_data = {\n",
        "            'subjects': subjects,\n",
        "            'intrests': interests,\n",
        "            'overall_average': average,\n",
        "            'grade': grade,\n",
        "            'location': location\n",
        "        }\n",
        "\n",
        "        if not all_programs_detailed_data:\n",
        "            return \"Warning: No program data was scraped yet.\"\n",
        "\n",
        "        return generate_chatbot_response(user_data, all_programs_detailed_data)\n",
        "\n",
        "    interface = gr.Interface(\n",
        "        fn=web_wrapper,\n",
        "        inputs=[\n",
        "            gr.Textbox(label=\"Current Subjects\"),\n",
        "            gr.Textbox(label=\"Interests\"),\n",
        "            gr.Textbox(label=\"Overall Average\"),\n",
        "            gr.Textbox(label=\"Grade\"),\n",
        "            gr.Textbox(label=\"Location\")\n",
        "        ],\n",
        "        outputs=gr.Markdown(label=\"Response\"),\n",
        "        title=\"University Application Helper\",\n",
        "        description=\"Scraped data is ready. Enter your info below.\"\n",
        "    )\n",
        "\n",
        "    interface.launch(share=True, debug=True)\n",
        "\n",
        "except ImportError:\n",
        "    print(\"Gradio is not installed. Please run '!pip install gradio' in a cell above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qv6adRHbnegn",
        "outputId": "ab9b6184-a6b3-4c85-992d-247462e4c9e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I'm a student assistant chatbot.\n",
            "Step 2: Building Master List of Programs (This loops through A-Z)...\n",
            "Fetching Group: A... Found 92 programs.\n",
            "Fetching Group: B... Found 129 programs.\n",
            "Fetching Group: C... Found 278 programs.\n",
            "Fetching Group: D-E... Found 160 programs.\n",
            "Fetching Group: F-G... Found 99 programs.\n",
            "Fetching Group: H... Found 77 programs.\n",
            "Fetching Group: I... Found 66 programs.\n",
            "Fetching Group: J-L... Found 71 programs.\n",
            "Fetching Group: M... Found 117 programs.\n",
            "Fetching Group: N-P... Found 155 programs.\n",
            "Fetching Group: Q-S... Found 100 programs.\n",
            "Fetching Group: T-Z... Found 54 programs.\n",
            "\n",
            "Total Programs Found: 1398\n",
            "\n",
            "Step 3: Starting parallel scrape of 1398 programs...\n",
            "Scraped 50/1398 programs...\n",
            "Scraped 100/1398 programs...\n",
            "Scraped 150/1398 programs...\n",
            "Scraped 200/1398 programs...\n",
            "Scraped 250/1398 programs...\n",
            "Scraped 300/1398 programs...\n",
            "Scraped 350/1398 programs...\n",
            "Scraped 400/1398 programs...\n",
            "Scraped 450/1398 programs...\n",
            "Scraped 500/1398 programs...\n",
            "Scraped 550/1398 programs...\n",
            "Scraped 600/1398 programs...\n",
            "Scraped 650/1398 programs...\n",
            "Scraped 700/1398 programs...\n",
            "Scraped 750/1398 programs...\n",
            "Scraped 800/1398 programs...\n",
            "Scraped 850/1398 programs...\n",
            "Scraped 900/1398 programs...\n",
            "Scraped 950/1398 programs...\n",
            "Scraped 1000/1398 programs...\n",
            "Scraped 1050/1398 programs...\n",
            "Scraped 1100/1398 programs...\n",
            "Scraped 1150/1398 programs...\n",
            "Scraped 1200/1398 programs...\n",
            "Scraped 1250/1398 programs...\n",
            "Scraped 1300/1398 programs...\n",
            "Scraped 1350/1398 programs...\n",
            "\n",
            "Finished. Successfully loaded 1398 programs.\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4c8d37d5133804a223.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4c8d37d5133804a223.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://4c8d37d5133804a223.gradio.live\n"
          ]
        }
      ]
    }
  ]
}